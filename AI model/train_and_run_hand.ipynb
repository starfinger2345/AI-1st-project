{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85f12670-d888-43bc-8598-372ab90ba30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1759043745.034221 59938343 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M1 Pro\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1759043745.061238 60007289 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1759043745.069761 60007289 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- knn 모델을 생성 & 학습 시작 ---\n",
      "\n",
      "--- 학습 완료 ---\n",
      "총 1364개의 샘플 데이터로 모델을 학습했습니다.\n",
      "데이터의 특징(feature) 차원으로 추출한 관절 각도의 개수: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 16:15:46.189 python3.9[73984:59938343] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "웹캠이 활성화되었습니다. 'q'를 눌러 종료하세요.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1759043749.705496 60007289 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ㅜ\n",
      "ㅜ\n",
      "ㅜ\n",
      "ㅜ\n",
      "ㅜ\n",
      "ㅜ\n",
      "space\n",
      "ㅕ\n",
      "ㅗ\n",
      "space\n",
      "\n",
      "--- 최종 인식된 제스처 목록 ---\n",
      "['ㅜ', 'ㅜ', 'ㅜ', 'ㅜ', 'ㅜ', 'ㅜ', 'space', 'ㅕ', 'ㅗ', 'space']\n"
     ]
    }
   ],
   "source": [
    "# 프레임워크: OpenCV\n",
    "# 머신러닝 알고리즘: KNN\n",
    "\n",
    "import cv2 # 웹캠 제어 및 ML 사용\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from PIL import ImageFont, ImageDraw, Image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import deque\n",
    "import time\n",
    "\n",
    "\n",
    "consonant_labels = ['ㄱ', 'ㄴ', 'ㄷ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅅ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "vowel_labels = ['ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅛ', 'ㅜ', 'ㅠ', 'ㅡ','ㅣ','ㅘ','ㅚ', 'ㅙ', 'ㅝ','ㅟ','ㅞ','ㅢ']\n",
    "command_labels = ['shift', 'space', 'end'] # '쌍자음 만들기' 명령어만 남김\n",
    "labels = consonant_labels + vowel_labels + command_labels\n",
    "\n",
    "#font_path = \"/Users/hwi/Library/Fonts/GowunDodum-Regular.ttf\"\n",
    "font_path = \"C:/Windows/Fonts/hmfmmuex.ttc\"\n",
    "dataset_file = 'combined_hand_landmark.txt'\n",
    "\n",
    "history = deque(maxlen=5)\n",
    "\n",
    "\n",
    "# 인덱스를 라벨로 변환하기 위한 맵 생성\n",
    "label_map = {i: label for i, label in enumerate(labels)}\n",
    "\n",
    "\n",
    "def putText_korean(image, text, pos, font_path, font_size, color):\n",
    "    \"\"\"\n",
    "    한글 텍스트를 화면에 표시.\n",
    "    \"\"\"\n",
    "    # OpenCV 이미지를 PIL 이미지로 변환\n",
    "    img_pil = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    draw = ImageDraw.Draw(img_pil)\n",
    "    # 폰트 로드\n",
    "    font = ImageFont.truetype(font_path, font_size)\n",
    "    # 텍스트 그리기\n",
    "    draw.text(pos, text, font=font, fill=tuple(color[::-1]))\n",
    "    # PIL 이미지를 다시 OpenCV 이미지로 변환하여 반환\n",
    "    return cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "\n",
    "# >> mediapipe Hands 모델 로드 <<\n",
    "mp_hands = mp.solutions.hands # 웹캠 영상에서 손가락 마디와 포인트를 그릴 수 있게 도와주는 유틸리티\n",
    "mp_drawing = mp.solutions.drawing_utils # 웹캠 영상에서 손가락 마디와 포인트를 그릴 수 있게 도와주는 유틸리티\n",
    "\n",
    "\n",
    "# >> mediapipe Hands 모델 초기화 <<\n",
    "hands = mp_hands.Hands(\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "\n",
    "# >> 각도 계산 <<\n",
    "def calculate_angles(joint):\n",
    "    \"\"\"\n",
    "    랜드마크 위치를 사용하여 관절 간의 각도를 계산.\n",
    "    joint가 (21,3) ndarray 형식으로 들어와서,\n",
    "    (x, y, z) 랜드마크 21개를 기반으로 15개 관절 각도를 반환.\n",
    "    \"\"\"\n",
    "    v1 = joint[[0,1,2,3,0,5,6,7,0,9,10,11,0,13,14,15,0,17,18,19],:]\n",
    "    v2 = joint[[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],:]\n",
    "    v = v2 - v1\n",
    "\n",
    "    # 벡터 정규화\n",
    "    v = v / np.linalg.norm(v, axis=1)[:, np.newaxis]\n",
    "\n",
    "    # 내적과 arccos를 사용해 각도 계산\n",
    "    angle = np.arccos(np.einsum('nt,nt->n',\n",
    "        v[[0,1,2,4,5,6,8,9,10,12,13,14,16,17,18],:],\n",
    "        v[[1,2,3,5,6,7,9,10,11,13,14,15,17,18,19],:]))\n",
    "    \n",
    "    # 라디안을 각도로 변환\n",
    "    angle = np.degrees(angle)\n",
    "\n",
    "    return angle.astype(np.float32)\n",
    "\n",
    "    \n",
    "\n",
    "# >> 데이터 로드 & 전처리 <<\n",
    "def load_and_preprocess(dataset_file):\n",
    "    labels = np.genfromtxt(dataset_file, delimiter=',', skip_header=1, usecols = 0, encoding =\"EUC-KR\", dtype = str)\n",
    "    angles = np.genfromtxt(dataset_file, delimiter=',', skip_header=1,usecols= range(1,127), encoding = \"EUC-KR\").astype(np.float32) # **각 제스처들의 라벨과 각도가 저장되어 있음, 정확도를 높이고 싶으면 데이터를 추가해보자!**\n",
    "\n",
    "    all_angles = []\n",
    "    for row in angles:\n",
    "        lh_landmarks = row[:63].reshape(21,3) # 총 21개의 landmark, 각각 총 3개의 (x,y,z) 좌표\n",
    "        rh_landmarks = row[63:].reshape(21,3)\n",
    "\n",
    "        lh_angles = calculate_angles(lh_landmarks) if np.any(lh_landmarks) else np.zeros(15)\n",
    "        rh_angles = calculate_angles(rh_landmarks) if np.any(rh_landmarks) else np.zeros(15)\n",
    "\n",
    "        all_angles.append(np.concatenate([lh_angles, rh_angles]))\n",
    "        \n",
    "    all_angles = np.array(all_angles, dtype = np.float32)\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    encoded_labels = encoder.fit_transform(labels)\n",
    "    \n",
    "\n",
    "    return all_angles, encoded_labels, encoder\n",
    "\n",
    "\n",
    "\n",
    "# >> 모델 학습 << \n",
    "def train_model(dataset_file):\n",
    "    \"\"\"\n",
    "    수집한 데이터를 knn 모델로 학습.\n",
    "    \"\"\"\n",
    "    # 전처리한 결과 로드\n",
    "    X, y, encoder = load_and_preprocess(dataset_file)\n",
    "    \n",
    "    # knn 모델 생성\n",
    "    knn = cv2.ml.KNearest_create()\n",
    "    print(\"--- knn 모델을 생성 & 학습 시작 ---\")\n",
    "    \n",
    "    # 모델 학습\n",
    "    knn.train(X, cv2.ml.ROW_SAMPLE, y.astype(np.int32))\n",
    "    print(\"\\n--- 학습 완료 ---\")\n",
    "    print(f\"총 {X.shape[0]}개의 샘플 데이터로 모델을 학습했습니다.\")\n",
    "    print(f\"데이터의 특징(feature) 차원으로 추출한 관절 각도의 개수: {X.shape[1]}\")\n",
    "    return knn, encoder\n",
    "\n",
    "\n",
    "# >> 실시간 프레임 처리 <<\n",
    "\n",
    "def run_frame(trained_knn_model, encoder):\n",
    "    \"\"\"\n",
    "    학습된 knn 모델을 사용해서 웹캠에서 실시간으로 제스처 인식.\n",
    "    \"\"\"\n",
    "    global histroy\n",
    "    \n",
    "    if trained_knn_model is None:\n",
    "        print(\"학습된 모델이 없어 검증을 시작할 수 없습니다.\")\n",
    "        return\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"오류: 웹캠을 열 수 없습니다.\")\n",
    "        return\n",
    "\n",
    "    print(\"웹캠이 활성화되었습니다. 'q'를 눌러 종료하세요.\")\n",
    "    \n",
    "    entered_string = []\n",
    "    display_label = \"\"\n",
    "    display_start_time = None\n",
    "    display_duration = 2 # 해당 label을 웹캠 화면에 표시할 시간(초)\n",
    "    \n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"프레임을 찾을 수 없습니다.\")\n",
    "            break\n",
    "\n",
    "        # 프레임을 수평으로, rgb로 변환\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # 손 정보를 처리\n",
    "        result = hands.process(frame_rgb)\n",
    "\n",
    "        # 다시 brg로 변환\n",
    "        #frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # 변수 초기화\n",
    "        predicted_label = \"\" # 예측 할 때마다 초기화\n",
    "        guide_text = \"손을 보여주세요\"\n",
    "\n",
    "        # 손 개수가 감지되었는지 확인\n",
    "        if result.multi_hand_landmarks:\n",
    "            left_hand, right_hand = None, None\n",
    "\n",
    "            for i, hand_landmarks in enumerate(result.multi_hand_landmarks):\n",
    "                handedness_label = result.multi_handedness[i].classification[0].label\n",
    "                joint = np.array([[lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark])\n",
    "\n",
    "                if handedness_label == \"Left\":\n",
    "                    left_hand = calculate_angles(joint)\n",
    "                elif handedness_label == \"Right\":\n",
    "                    right_hand = calculate_angles(joint)\n",
    "\n",
    "            if left_hand is None:\n",
    "                left_hand = np.zeros(15)\n",
    "            if right_hand is None:\n",
    "                right_hand = np.zeros(15)\n",
    "\n",
    "            feature_vector = np.concatenate([left_hand, right_hand]).reshape(1,-1).astype(np.float32)\n",
    "\n",
    "            if not np.all(np.isnan(feature_vector)): # 아무 것도 인식되지 않는게 아니라면(인식되는 손이 있다면)\n",
    "                try:\n",
    "                    # ret, results, neighbours, dist\n",
    "                    _, results, _, _ = trained_knn_model.findNearest(feature_vector, k=3)\n",
    "                    predicted_label = encoder.inverse_transform([int(results[0][0])])[0]\n",
    "                    predicted_text = f\"{'왼손' if handedness_label == 'Left' else '오른손'}: {predicted_label}\"\n",
    "\n",
    "                    # 5번 연속으로 같은 동작으로 인식되면 해당 동작을 인식\n",
    "                    history.append(predicted_label)\n",
    "                    if len(history) == 5 and len(set(history)) == 1:  # 최근 5개가 동일하면\n",
    "                        entered_string.append(predicted_label)\n",
    "                        print(predicted_label)\n",
    "                        history.clear()\n",
    "\n",
    "                        # 인식된 동작을 웹캠 화면에 표시\n",
    "                        display_label = predicted_label\n",
    "                        display_start_time = time.time()\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    guide_text = \"알 수 없는 제스처\"\n",
    "            else:\n",
    "                guide_text = \"손이 인식되지 않습니다.\"\n",
    "            # 화면에 랜드마크 그리기\n",
    "            for res in result.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(frame, res, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        # 웹캠 화면에 띄울 텍스트\n",
    "        # 제스처 인식되면 일정 시간 동안 해당 문자를, 그렇지 않다면 안내 문구를 표시\n",
    "        if display_start_time and ((time.time()-display_start_time) < display_duration):\n",
    "            display_text = display_label\n",
    "        else:\n",
    "            display_text = guide_text\n",
    "\n",
    "        \n",
    "        # 텍스트 위치 계산 및 표시\n",
    "        text_size = 50 # 폰트 크기\n",
    "        font = ImageFont.truetype(font_path, text_size)\n",
    "        bbox = font.getbbox(guide_text)  # (xmin, ymin, xmax, ymax)\n",
    "        text_w = bbox[2] - bbox[0]\n",
    "        text_h = bbox[3] - bbox[1]\n",
    "            \n",
    "        text_x = int((frame.shape[1] - text_w) / 2)\n",
    "        text_y = int(frame.shape[0] - 50)\n",
    "            \n",
    "        frame = putText_korean(frame, display_text, (text_x, text_y), font_path, text_size, (0, 255, 0))\n",
    "            \n",
    "        cv2.imshow('frame', frame)\n",
    "            \n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "    return entered_string\n",
    "            \n",
    "            \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dataset_file = 'combined_hand_landmark.txt'\n",
    "    \n",
    "    knn, encoder = train_model(dataset_file)\n",
    "    \n",
    "    input_string = run_frame(knn, encoder)\n",
    "\n",
    "    print(\"\\n--- 최종 인식된 제스처 목록 ---\")\n",
    "    print(input_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3314110-60e4-4243-bcb7-58e09482373c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- k 값에 따른 정확도 평가를 시작합니다. ---\n",
      "\n",
      "--- k = 1 ---\n",
      "정확도: 0.9560\n",
      "\n",
      "--- k = 2 ---\n",
      "정확도: 0.9194\n",
      "\n",
      "--- k = 3 ---\n",
      "정확도: 0.9194\n",
      "\n",
      "--- k = 4 ---\n",
      "정확도: 0.8974\n",
      "\n",
      "--- k = 5 ---\n",
      "정확도: 0.8938\n",
      "\n",
      "--- k = 6 ---\n",
      "정확도: 0.8828\n",
      "\n",
      "--- k = 7 ---\n",
      "정확도: 0.8938\n",
      "\n",
      "--- k = 8 ---\n",
      "정확도: 0.8791\n",
      "\n",
      "--- k = 9 ---\n",
      "정확도: 0.8828\n",
      "\n",
      "--- k = 10 ---\n",
      "정확도: 0.8645\n",
      "\n",
      "--- K 값별 최종 정확도 ---\n",
      "K = 1: 정확도 = 0.9560\n",
      "K = 2: 정확도 = 0.9194\n",
      "K = 3: 정확도 = 0.9194\n",
      "K = 4: 정확도 = 0.8974\n",
      "K = 5: 정확도 = 0.8938\n",
      "K = 6: 정확도 = 0.8828\n",
      "K = 7: 정확도 = 0.8938\n",
      "K = 8: 정확도 = 0.8791\n",
      "K = 9: 정확도 = 0.8828\n",
      "K = 10: 정확도 = 0.8645\n"
     ]
    }
   ],
   "source": [
    "# >> K값 변화에 따른 모델 정확도 확인 <<\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "def calculate_angles(joint):\n",
    "    \"\"\"\n",
    "    랜드마크 위치를 사용하여 관절 간의 각도를 계산.\n",
    "    joint가 (21,3) ndarray 형식으로 들어와서,\n",
    "    (x, y, z) 랜드마크 21개를 기반으로 15개 관절 각도를 반환.\n",
    "    \"\"\"\n",
    "    v1 = joint[[0,1,2,3,0,5,6,7,0,9,10,11,0,13,14,15,0,17,18,19],:]\n",
    "    v2 = joint[[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],:]\n",
    "    v = v2 - v1\n",
    "\n",
    "    # 벡터 정규화\n",
    "    v = v / np.linalg.norm(v, axis=1)[:, np.newaxis]\n",
    "\n",
    "    # 내적과 arccos를 사용해 각도 계산\n",
    "    angle = np.arccos(np.einsum('nt,nt->n',\n",
    "        v[[0,1,2,4,5,6,8,9,10,12,13,14,16,17,18],:],\n",
    "        v[[1,2,3,5,6,7,9,10,11,13,14,15,17,18,19],:]))\n",
    "    \n",
    "    # 라디안을 각도로 변환\n",
    "    angle = np.degrees(angle)\n",
    "\n",
    "    return angle.astype(np.float32)\n",
    "\n",
    "\n",
    "# >> 데이터 로드 & 전처리 <<\n",
    "def load_and_preprocess(dataset_file):\n",
    "    labels = np.genfromtxt(dataset_file, delimiter=',', skip_header=1, usecols = 0, encoding =\"EUC-KR\", dtype = str)\n",
    "    angles = np.genfromtxt(dataset_file, delimiter=',', skip_header=1,usecols= range(1,127), encoding = \"EUC-KR\").astype(np.float32) # **각 제스처들의 라벨과 각도가 저장되어 있음, 정확도를 높이고 싶으면 데이터를 추가해보자!**\n",
    "\n",
    "    all_angles = []\n",
    "    for row in angles:\n",
    "        lh_landmarks = row[:63].reshape(21,3) # 총 21개의 landmark, 각각 총 3개의 (x,y,z) 좌표\n",
    "        rh_landmarks = row[63:].reshape(21,3)\n",
    "\n",
    "        lh_angles = calculate_angles(lh_landmarks) if np.any(lh_landmarks) else np.zeros(15)\n",
    "        rh_angles = calculate_angles(rh_landmarks) if np.any(rh_landmarks) else np.zeros(15)\n",
    "\n",
    "        all_angles.append(np.concatenate([lh_angles, rh_angles]))\n",
    "        \n",
    "    all_angles = np.array(all_angles, dtype = np.float32)\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    encoded_labels = encoder.fit_transform(labels)\n",
    "    \n",
    "\n",
    "    return all_angles, encoded_labels, encoder\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_knn_accuracy(dataset_file):\n",
    "    \"\"\"\n",
    "    1부터 10까지의 k 값에 따른 KNN 모델의 정확도를 평가합니다.\n",
    "    \"\"\"\n",
    "    X, y, encoder = load_and_preprocess(dataset_file)\n",
    "    \n",
    "    # 훈련 세트와 테스트 세트로 분할 (8:2 비율)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    accuracies = []\n",
    "    \n",
    "    # k 값을 1부터 10까지 반복\n",
    "    for k in range(1, 11):\n",
    "        print(f\"\\n--- k = {k} ---\")\n",
    "        knn = cv2.ml.KNearest_create()\n",
    "        knn.setDefaultK(k)\n",
    "        \n",
    "        # 모델 학습\n",
    "        knn.train(X_train, cv2.ml.ROW_SAMPLE, y_train.astype(np.int32))\n",
    "        \n",
    "        # 테스트 데이터로 예측\n",
    "        # k=k를 명시하여 현재 k값을 사용하도록 함\n",
    "        _, results, _, _ = knn.findNearest(X_test, k=k)\n",
    "        \n",
    "        # 예측된 라벨과 실제 라벨 비교\n",
    "        y_pred = results.flatten()\n",
    "        \n",
    "        # 정확도 계산\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append((k, accuracy))\n",
    "        print(f\"정확도: {accuracy:.4f}\")\n",
    "\n",
    "    return accuracies\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    dataset_file = 'combined_hand_landmark.txt'\n",
    "\n",
    "    # k 값에 따른 정확도 평가\n",
    "    print(\"--- k 값에 따른 정확도 평가를 시작합니다. ---\")\n",
    "    k_accuracies = evaluate_knn_accuracy(dataset_file)\n",
    "    \n",
    "    print(\"\\n--- K 값별 최종 정확도 ---\")\n",
    "    for k, acc in k_accuracies:\n",
    "        print(f\"K = {k}: 정확도 = {acc:.4f}\")\n",
    "    \n",
    "    # 여기서 최적의 k 값을 선택하여 실제 모델을 학습하고 run_frame 실행\n",
    "    # best_k = max(k_accuracies, key=lambda item: item[1])[0]\n",
    "    # print(f\"\\n최적의 K 값은 {best_k} 입니다.\")\n",
    "    \n",
    "    # 최적의 k 값으로 최종 모델 학습 및 실시간 제스처 인식\n",
    "    # knn, encoder = train_model(dataset_file)\n",
    "    # input_string = run_frame(knn, encoder)\n",
    "    # print(\"\\n--- 최종 인식된 제스처 목록 ---\")\n",
    "    # print(input_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
